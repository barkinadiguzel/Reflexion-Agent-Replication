# ğŸ¤– Reflexion-Agent-Replication â€” Self-Reflective Vision Agent

This repository provides a **PyTorch-based research replication** of  
**Reflexion: Language Agents with Verbal Reinforcement Learning â€” Nakano et al., 2023**,  
adapted into a **vision reasoning framework**.

The project translates the paperâ€™s **self-reflection loop, memory integration, and reward evaluation**
into a modular vision agent pipeline.

- Enables **iterative multi-step reasoning over visual features** ğŸ§©  
- Integrates **short-term and long-term memory for reflection** ğŸª  
- Supports **trajectory evaluation and reward-driven self-improvement** âš¡  

**Paper reference:**  [Reflexion: Language Agents with Verbal Reinforcement Learning â€” Nakano et al., 2023](https://arxiv.org/abs/2303.11366) ğŸ“„

---

## ğŸŒŒ Overview â€” Reflexion Vision Pipeline

![Reflexion Overview](images/figmix.jpg)

The core idea:

> Reasoning emerges from reflection, not from a single forward pass.

Instead of mapping an image directly to a prediction:

$$
x \longrightarrow y
$$

We reformulate inference as a **self-reflective search process**:

$$
s_0 \rightarrow s_1 \rightarrow s_2 \rightarrow \dots \rightarrow s_T
$$

where each state $s_t$ encodes a **trajectory of reasoning with past reflections**.

The final prediction is obtained by selecting the action with the highest expected reward:

$$
y = H(s_T^*)
$$

with

$$
s_T^* = \arg\max_{s_T \in \mathcal{S}_T} R(s_T)
$$

Here, $R(s_T)$ is computed by the **evaluator**, integrating both task success and reflection-based learning. (We did not implement that part.)

---

## ğŸ”¬ Mathematical Formulation

Let an input image be encoded as

$$
z_0 = E(x)
$$

where  
- $x$ is the input image  
- $E(\cdot)$ is a vision encoder (CNN / ViT / CLIP)  
- $z_0$ is the initial reasoning state  

At each reasoning step $t$, the agent produces an action $a_t$ using its memory-informed policy:

$$
a_t \sim \pi_\theta(a \mid s_t, M)
$$

where $M$ is the **long-term reflection memory**.

The environment returns a reward signal:

$$
r_t = R(s_t, a_t)
$$

After a trajectory of length $T$, the agent generates a **reflection**:

$$
f(\tau, r, M) = \text{ReflectionGenerator}(\tau, r, M)
$$

This reflection updates the long-term memory:

$$
M \leftarrow M \cup f(\tau, r, M)
$$

---

## ğŸ§  What the Model Learns

- To evaluate its own reasoning trajectories ğŸª  
- To leverage past reflections in generating next actions ğŸ”„  
- To assign credit via reward for correct visual reasoning âš¡  
- To suppress premature or low-value decisions âŒ  
- To gradually improve through self-reflective iterations ğŸŒ±  

The inference loop becomes a **reward-driven, reflection-augmented reasoning process** rather than a single-shot mapping.

---

## ğŸ“¦ Repository Structure

```bash
Reflexion-Agent-Replication/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ actor/
â”‚   â”‚   â””â”€â”€ actor.py                  # State â†’ Action / Generation (LLM policy)
â”‚   â”‚
â”‚   â”œâ”€â”€ evaluator/
â”‚   â”‚   â””â”€â”€ evaluator.py              # Trajectory â†’ Reward / Pass-Fail / Score
â”‚   â”‚
â”‚   â”œâ”€â”€ reflection/
â”‚   â”‚   â””â”€â”€ reflection_generator.py   # (Trajectory, Reward, Memory) â†’ Reflection text
â”‚   â”‚
â”‚   â”œâ”€â”€ memory/
â”‚   â”‚   â”œâ”€â”€ short_term.py             # Trajectory buffer (current episode)
â”‚   â”‚   â””â”€â”€ long_term.py              # Reflection memory (episodic memory)
â”‚   â”‚
â”‚   â”œâ”€â”€ environment/
â”‚   â”‚   â””â”€â”€ env_interface.py          # Task wrapper (game, vision, reasoning, etc.)
â”‚   â”‚
â”‚   â”œâ”€â”€ policy/
â”‚   â”‚   â””â”€â”€ policy.py                 # Ï€Î¸ = Actor + Memory (prompt builder)
â”‚   â”‚
â”‚   â”œâ”€â”€ pipeline/
â”‚   â”‚   â””â”€â”€ reflexion_pipeline.py     # Full Reflexion loop controller
â”‚   â”‚
â”‚   â””â”€â”€ config.py                     # MAX_TRIALS, MEMORY_SIZE, MODEL_NAME
â”‚
â”œâ”€â”€ images/
â”‚   â””â”€â”€ figmix.jpg                     
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```
---


## ğŸ”— Feedback

For questions or feedback, contact: [barkin.adiguzel@gmail.com](mailto:barkin.adiguzel@gmail.com)
